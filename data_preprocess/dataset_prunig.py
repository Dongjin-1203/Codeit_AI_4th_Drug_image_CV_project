import os
import json
import glob
import shutil
import random
from pathlib import Path
from collections import defaultdict, Counter

def create_small_dataset(
    source_path="./data", 
    output_path="./data/small_data",
    train_size=200, 
    test_size=100,
    sampling_strategy="balanced"
):
    """
    ì†Œê·œëª¨ ë°ì´í„°ì…‹ ì¶”ì¶œ (ë””ë ‰í† ë¦¬ êµ¬ì¡° ìœ ì§€)
    
    Args:
        source_path: ì›ë³¸ ë°ì´í„°ì…‹ ê²½ë¡œ
        output_path: ì†Œê·œëª¨ ë°ì´í„°ì…‹ ì €ì¥ ê²½ë¡œ
        train_size: ì¶”ì¶œí•  í›ˆë ¨ ë°ì´í„° ìˆ˜
        test_size: ì¶”ì¶œí•  í…ŒìŠ¤íŠ¸ ë°ì´í„° ìˆ˜
        sampling_strategy: "balanced" | "random" | "quality"
    """
    
    print("ğŸ”§ ì†Œê·œëª¨ ë°ì´í„°ì…‹ ì¶”ì¶œ ì‹œì‘...")
    print(f"ğŸ“Š ëª©í‘œ: Train {train_size}ê°œ, Test {test_size}ê°œ")
    print(f"ğŸ“ ì¶œë ¥ ê²½ë¡œ: {output_path}")
    print("=" * 50)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    setup_output_directories(output_path)
    
    # 1. í›ˆë ¨ ë°ì´í„° ì¶”ì¶œ
    train_pairs = extract_train_data(source_path, train_size, sampling_strategy)
    copy_train_data(train_pairs, source_path, output_path)
    
    # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ
    test_files = extract_test_data(source_path, test_size)
    copy_test_data(test_files, source_path, output_path)
    
    # 3. ê²°ê³¼ í™•ì¸
    verify_small_dataset(output_path)
    
    print("âœ… ì†Œê·œëª¨ ë°ì´í„°ì…‹ ì¶”ì¶œ ì™„ë£Œ!")

def setup_output_directories(output_path):
    """ì¶œë ¥ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ì •"""
    directories = [
        os.path.join(output_path, "train_images"),
        os.path.join(output_path, "test_images"),
        os.path.join(output_path, "train_annotations")
    ]
    
    for dir_path in directories:
        os.makedirs(dir_path, exist_ok=True)
    
    print("ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ")

def extract_train_data(source_path, train_size, sampling_strategy):
    """í›ˆë ¨ ë°ì´í„° ì¶”ì¶œ (ì´ë¯¸ì§€-ì–´ë…¸í…Œì´ì…˜ ë§¤ì¹­)"""
    print("ğŸ” í›ˆë ¨ ë°ì´í„° ë¶„ì„ ì¤‘...")
    
    # ëª¨ë“  í›ˆë ¨ ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
    train_images = glob.glob(os.path.join(source_path, "train_images", "*.png"))
    
    # ì´ë¯¸ì§€-ì–´ë…¸í…Œì´ì…˜ ë§¤ì¹­ í™•ì¸
    valid_pairs = []
    annotation_folders = glob.glob(os.path.join(source_path, "train_annotations", "*"))
    
    for img_path in train_images:
        img_name = os.path.basename(img_path).replace('.png', '')
        
        # í•´ë‹¹í•˜ëŠ” JSON íŒŒì¼ ì°¾ê¸°
        json_path = find_matching_annotation(img_name, annotation_folders)
        
        if json_path:
            valid_pairs.append({
                'image_path': img_path,
                'annotation_path': json_path,
                'image_name': img_name
            })
    
    print(f"ğŸ“Š ë§¤ì¹­ëœ ì´ë¯¸ì§€-ì–´ë…¸í…Œì´ì…˜ ìŒ: {len(valid_pairs)}ê°œ")
    
    # ìƒ˜í”Œë§ ì „ëµì— ë”°ë¼ ì„ íƒ
    if sampling_strategy == "balanced":
        selected_pairs = balanced_sampling(valid_pairs, train_size)
    elif sampling_strategy == "quality":
        selected_pairs = quality_sampling(valid_pairs, train_size)
    else:  # random
        selected_pairs = random.sample(valid_pairs, min(train_size, len(valid_pairs)))
    
    print(f"âœ… {len(selected_pairs)}ê°œ í›ˆë ¨ ìƒ˜í”Œ ì„ íƒ ì™„ë£Œ")
    return selected_pairs

def find_matching_annotation(img_name, annotation_folders):
    """ì´ë¯¸ì§€ì— ë§¤ì¹­ë˜ëŠ” ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ ì°¾ê¸°"""
    for folder in annotation_folders:
        # ê° í•˜ìœ„ í´ë”ì—ì„œ JSON íŒŒì¼ ì°¾ê¸°
        json_files = glob.glob(os.path.join(folder, "*", f"{img_name}.json"))
        if json_files:
            return json_files[0]
    return None

def balanced_sampling(valid_pairs, target_size):
    """ì•Œì•½ ì½”ë“œë³„ ê· ë“± ìƒ˜í”Œë§"""
    print("âš–ï¸ ê· ë“± ìƒ˜í”Œë§ ì ìš© ì¤‘...")
    
    # ì•Œì•½ ì½”ë“œë³„ë¡œ ê·¸ë£¹í•‘
    pill_groups = defaultdict(list)
    
    for pair in valid_pairs:
        # íŒŒì¼ëª…ì—ì„œ ì•Œì•½ ì½”ë“œ ì¶”ì¶œ (ì²« ë²ˆì§¸ K- ì½”ë“œ)
        filename = pair['image_name']
        try:
            pill_code = filename.split('-')[1]  # K-003544ì—ì„œ 003544 ì¶”ì¶œ
            pill_groups[pill_code].append(pair)
        except:
            pill_groups['unknown'].append(pair)
    
    print(f"ğŸ·ï¸ ë°œê²¬ëœ ì•Œì•½ ì½”ë“œ: {len(pill_groups)}ê°œ")
    
    # ê° ê·¸ë£¹ì—ì„œ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
    samples_per_group = max(1, target_size // len(pill_groups))
    selected_pairs = []
    
    for code, pairs in pill_groups.items():
        sample_count = min(samples_per_group, len(pairs))
        selected = random.sample(pairs, sample_count)
        selected_pairs.extend(selected)
        print(f"  {code}: {sample_count}ê°œ ì„ íƒ")
    
    # ëª©í‘œ ìˆ˜ì— ë§ê²Œ ì¡°ì •
    if len(selected_pairs) < target_size:
        remaining = target_size - len(selected_pairs)
        remaining_pairs = [p for p in valid_pairs if p not in selected_pairs]
        additional = random.sample(remaining_pairs, min(remaining, len(remaining_pairs)))
        selected_pairs.extend(additional)
    elif len(selected_pairs) > target_size:
        selected_pairs = random.sample(selected_pairs, target_size)
    
    return selected_pairs

def quality_sampling(valid_pairs, target_size):
    """í’ˆì§ˆ ê¸°ë°˜ ìƒ˜í”Œë§ (íŒŒì¼ í¬ê¸° ê³ ë ¤)"""
    print("ğŸ¯ í’ˆì§ˆ ê¸°ë°˜ ìƒ˜í”Œë§ ì ìš© ì¤‘...")
    
    # íŒŒì¼ í¬ê¸° ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    pairs_with_size = []
    for pair in valid_pairs:
        file_size = os.path.getsize(pair['image_path'])
        pairs_with_size.append((pair, file_size))
    
    # íŒŒì¼ í¬ê¸° ìƒìœ„ ì„ íƒ (í° íŒŒì¼ = ê³ í’ˆì§ˆ ê°€ì •)
    pairs_with_size.sort(key=lambda x: x[1], reverse=True)
    selected_pairs = [pair for pair, _ in pairs_with_size[:target_size]]
    
    print(f"ğŸ“ˆ ìƒìœ„ {len(selected_pairs)}ê°œ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ì„ íƒ")
    return selected_pairs

def copy_train_data(selected_pairs, source_path, output_path):
    """ì„ íƒëœ í›ˆë ¨ ë°ì´í„° ë³µì‚¬"""
    print("ğŸ“‹ í›ˆë ¨ ë°ì´í„° ë³µì‚¬ ì¤‘...")
    
    for i, pair in enumerate(selected_pairs):
        # ì´ë¯¸ì§€ ë³µì‚¬
        img_src = pair['image_path']
        img_dst = os.path.join(output_path, "train_images", os.path.basename(img_src))
        shutil.copy2(img_src, img_dst)
        
        # ì–´ë…¸í…Œì´ì…˜ ë³µì‚¬ (ë””ë ‰í† ë¦¬ êµ¬ì¡° ìœ ì§€)
        json_src = pair['annotation_path']
        
        # ì›ë³¸ ì–´ë…¸í…Œì´ì…˜ ê²½ë¡œì—ì„œ ìƒëŒ€ ê²½ë¡œ ì¶”ì¶œ
        rel_path = os.path.relpath(json_src, os.path.join(source_path, "train_annotations"))
        json_dst = os.path.join(output_path, "train_annotations", rel_path)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(json_dst), exist_ok=True)
        shutil.copy2(json_src, json_dst)
        
        if (i + 1) % 50 == 0:
            print(f"  ì§„í–‰ë¥ : {i + 1}/{len(selected_pairs)}")
    
    print(f"âœ… {len(selected_pairs)}ê°œ í›ˆë ¨ ë°ì´í„° ë³µì‚¬ ì™„ë£Œ")

def extract_test_data(source_path, test_size):
    """í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ"""
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„ íƒ ì¤‘...")
    
    test_images = glob.glob(os.path.join(source_path, "test_images", "*.png"))
    selected_files = random.sample(test_images, min(test_size, len(test_images)))
    
    print(f"âœ… {len(selected_files)}ê°œ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì„ íƒ ì™„ë£Œ")
    return selected_files

def copy_test_data(selected_files, source_path, output_path):
    """ì„ íƒëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³µì‚¬"""
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³µì‚¬ ì¤‘...")
    
    for img_src in selected_files:
        img_dst = os.path.join(output_path, "test_images", os.path.basename(img_src))
        shutil.copy2(img_src, img_dst)
    
    print(f"âœ… {len(selected_files)}ê°œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³µì‚¬ ì™„ë£Œ")

def verify_small_dataset(output_path):
    """ì¶”ì¶œëœ ë°ì´í„°ì…‹ ê²€ì¦"""
    print("\nğŸ” ë°ì´í„°ì…‹ ê²€ì¦ ì¤‘...")
    
    train_imgs = len(glob.glob(os.path.join(output_path, "train_images", "*.png")))
    test_imgs = len(glob.glob(os.path.join(output_path, "test_images", "*.png")))
    annotations = len(glob.glob(os.path.join(output_path, "train_annotations", "*", "*", "*.json")))
    
    print(f"ğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"  ğŸ‹ï¸ í›ˆë ¨ ì´ë¯¸ì§€: {train_imgs}ê°œ")
    print(f"  ğŸ§ª í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {test_imgs}ê°œ")
    print(f"  ğŸ“ ì–´ë…¸í…Œì´ì…˜: {annotations}ê°œ")
    
    # ë””ë ‰í† ë¦¬ êµ¬ì¡° ì¶œë ¥
    print(f"\nğŸ“ ìƒì„±ëœ ë””ë ‰í† ë¦¬ êµ¬ì¡°:")
    for root, dirs, files in os.walk(output_path):
        level = root.replace(output_path, '').count(os.sep)
        indent = ' ' * 2 * level
        print(f"{indent}{os.path.basename(root)}/")
        subindent = ' ' * 2 * (level + 1)
        if files:
            print(f"{subindent}íŒŒì¼ {len(files)}ê°œ")

# create_small_dataset íŒŒë¼ë¯¸í„°
"""
ê¸°í˜¸ì— ë§ê²Œ ì¡°ì •í•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”!
create_small_dataset í•¨ìˆ˜ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë³€ê²½í•˜ë©´ ë©ë‹ˆë‹¤.

# 1. ê¸°ë³¸ ì‚¬ìš©ë²• (ê· ë“± ìƒ˜í”Œë§)
create_small_dataset(
    source_path="./data",
    output_path="./data/small_data",
    train_size=200,
    test_size=100,
    sampling_strategy="balanced"
)

# 2. í’ˆì§ˆ ìš°ì„  ìƒ˜í”Œë§
create_small_dataset(
    source_path="./data",
    output_path="./data/small_data_quality",
    train_size=150,
    test_size=80,
    sampling_strategy="quality"
)

# 3. ì™„ì „ ëœë¤ ìƒ˜í”Œë§(í”„ë¡œí† íƒ€ì…ìš©)
create_small_dataset(
    source_path="./data",
    output_path="./data/small_data_prototype",
    train_size=300,
    test_size=150,
    sampling_strategy="random"
)

# 4. ì¤‘ê°„ê·œëª¨ ë°ì´í„°ì…‹ ìƒì„±(ìµœì¢… ê²€ì¦ìš©) 
create_small_dataset(
    source_path="./data",
    output_path="./data/small_data_meidium",
    train_size=750,
    test_size=400,
    sampling_strategy="random"
)
"""

def create_quick_small_dataset(train_count=200, test_count=100):
    """ì†Œê·œëª¨ ë°ì´í„°ì…‹ ìƒì„±ê¸°"""
    create_small_dataset(
        source_path="./data",
        output_path="./data/small_data",
        train_size=train_count,
        test_size=test_count,
        sampling_strategy="balanced"
    )
    print(f"ğŸ‰ ì†Œê·œëª¨ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ! './data/small_data' í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# ì‚¬ìš©ë²•: ê·¸ëƒ¥ ì‹¤í–‰í•˜ë©´ ë¨
# if __name__ == "__main__":
#     create_quick_small_dataset()